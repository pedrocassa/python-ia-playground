{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d34de5e7-db6a-45a5-afe1-ba81d8b04f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grafo criado com 24 n√≥s e 10 arestas.\n",
      "LARGE LANGUAGE MODELS -> DATASETS : Large language models are trained using large datasets that provide the diverse text data necessary for learning\n",
      "LARGE LANGUAGE MODELS -> DIFFERENTIAL PRIVACY : Differential privacy is applied to large language models to protect sensitive information in training data\n",
      "FEDERATED LEARNING -> HEALTHCARE : Federated learning is used in healthcare to train models on patient data without compromising privacy\n",
      "ADVERSARIAL TRAINING -> SECURITY MEASURES : Adversarial training is a security measure used to enhance the robustness of models against attacks\n",
      "LEARNING RATE -> ADAGRAD : AdaGrad adjusts the learning rate dynamically during model training\n",
      "LEARNING RATE -> RMSPROP : RMSprop is another optimizer that modifies the learning rate to improve training stability\n",
      "ADAPTER ARCHITECTURE -> LORA : LoRA is an alternative adapter architecture for efficient fine-tuning of large models\n",
      "DATA PREPROCESSING -> DATA IMBALANCE : Data preprocessing techniques are used to address data imbalance before model training\n",
      "GPU-BASED DEPLOYMENT -> HARDWARE ACCELERATORS : GPU-based deployment relies on hardware accelerators like GPUs to run large models efficiently\n",
      "DISTRIBUTED INFERENCE -> DEPLOYMENT : Distributed inference is a deployment strategy used to handle the computational load of large models\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# 1. L√™ o arquivo de entrada\n",
    "with open('entrada-processada.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# 2. Cria o grafo direcionado\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# 3. Processa cada linha\n",
    "for line in lines:\n",
    "    if line.startswith('(\"entity'):\n",
    "        # Exemplo: (\"entity\"{{tuple_delimiter}}LARGE LANGUAGE MODELS{{tuple_delimiter}}large language model{{tuple_delimiter}}Large language models (LLMs) are advanced AI models designed...\n",
    "        parts = line.split('{{tuple_delimiter}}')\n",
    "        entity_name = parts[1].strip()\n",
    "        entity_type = parts[2].strip()\n",
    "        entity_desc = parts[3].split(')')[0].strip()  # at√© fechar o par√™nteses\n",
    "\n",
    "        # Adiciona como n√≥ com atributos\n",
    "        G.add_node(entity_name, type=entity_type, description=entity_desc)\n",
    "\n",
    "    elif line.startswith('(\"relationship'):\n",
    "        # Exemplo: (\"relationship\"{{tuple_delimiter}}LARGE LANGUAGE MODELS{{tuple_delimiter}}DATASETS{{tuple_delimiter}}Large language models are trained using large datasets{{tuple_delimiter}}8\n",
    "        parts = line.split('{{tuple_delimiter}}')\n",
    "        src = parts[1].strip()\n",
    "        tgt = parts[2].strip()\n",
    "        relation_desc = parts[3].strip()\n",
    "\n",
    "        # Adiciona como aresta com descri√ß√£o\n",
    "        G.add_edge(src, tgt, description=relation_desc)\n",
    "\n",
    "print(f\"Grafo criado com {G.number_of_nodes()} n√≥s e {G.number_of_edges()} arestas.\")\n",
    "\n",
    "# (Opcional) 4. Exemplo: listar as rela√ß√µes\n",
    "for u, v, data in G.edges(data=True):\n",
    "    print(f\"{u} -> {v} : {data['description']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5e26ef6-c433-4e30-b534-dea4893ea09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import get_close_matches\n",
    "\n",
    "def buscar_no_grafo(pergunta, top_k=3):\n",
    "    # 1. Lista todos os n√≥s\n",
    "    todos_nos = list(G.nodes)\n",
    "    \n",
    "    # 2. Usa fuzzy match para encontrar n√≥s mais similares √† pergunta\n",
    "    matches = get_close_matches(pergunta.upper(), todos_nos, n=top_k, cutoff=0.1)\n",
    "    \n",
    "    contexto = []\n",
    "    for match in matches:\n",
    "        node_data = G.nodes[match]\n",
    "        contexto.append(f\"Entidade: {match} ({node_data['type']})\\nDescri√ß√£o: {node_data['description']}\")\n",
    "        \n",
    "        # 3. Pega rela√ß√µes de sa√≠da\n",
    "        for _, target, edge_data in G.out_edges(match, data=True):\n",
    "            contexto.append(f\"Relacionamento: {match} -> {target}\\nDescri√ß√£o: {edge_data['description']}\")\n",
    "        \n",
    "        # 4. Pega rela√ß√µes de entrada\n",
    "        for source, _, edge_data in G.in_edges(match, data=True):\n",
    "            contexto.append(f\"Relacionamento: {source} -> {match}\\nDescri√ß√£o: {edge_data['description']}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(contexto)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cccfe2d7-2fde-44f3-a9f9-65e633f38355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Usando dispositivo: cpu\n",
      "üîÑ Carregando modelo TinyLlama...\n",
      "‚úÖ Modelo TinyLlama carregado!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# ‚úÖ Detecta se h√° GPU (CUDA) dispon√≠vel\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üîé Usando dispositivo: {device}\")\n",
    "\n",
    "# ‚úÖ Passo 2: Carregar TinyLlama (sem device_map)\n",
    "print(\"üîÑ Carregando modelo TinyLlama...\")\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    ").to(device)\n",
    "print(\"‚úÖ Modelo TinyLlama carregado!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb913d3-06c3-4465-90f9-675bb4e76481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ùì Sua pergunta (ou 'sair'):  What is data privacy?\n"
     ]
    }
   ],
   "source": [
    "def responder_graphrag(pergunta_usuario):\n",
    "    # üîç Passo 1: Buscar contexto no grafo\n",
    "    contexto = buscar_no_grafo(pergunta_usuario)\n",
    "    \n",
    "    # üß† Passo 2: Montar prompt estilo RAG\n",
    "    prompt = f\"\"\"\n",
    "        Voc√™ √© um assistente especializado em intelig√™ncia artificial e privacidade.\n",
    "        \n",
    "        Use **exclusivamente** o contexto abaixo para responder √† pergunta do usu√°rio.\n",
    "        \n",
    "        Contexto:\n",
    "        {contexto}\n",
    "        \n",
    "        Pergunta: {pergunta_usuario}\n",
    "        \n",
    "        Resposta:\n",
    "    \"\"\"\n",
    "    # Tokeniza\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Gera resposta\n",
    "    outputs = model.generate(**inputs, max_new_tokens=300, temperature=0.7, do_sample=True)\n",
    "    resposta = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # S√≥ extrai a parte da resposta (remove o prompt inicial)\n",
    "    resposta_final = resposta.split(\"Resposta:\")[-1].strip()\n",
    "    return resposta_final\n",
    "\n",
    "# ===========================\n",
    "# 5. Teste interativo!\n",
    "# ===========================\n",
    "\n",
    "while True:\n",
    "    pergunta = input(\"\\n‚ùì Sua pergunta (ou 'sair'): \")\n",
    "    if pergunta.lower() == \"sair\":\n",
    "        break\n",
    "    resposta = responder_graphrag(pergunta)\n",
    "    print(\"\\nü§ñ Resposta do GraphRAG:\\n\", resposta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91749ab1-03c4-4833-8991-1f89cf97d832",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
